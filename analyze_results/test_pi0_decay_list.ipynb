{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67bbea79",
   "metadata": {},
   "source": [
    "# Pi0 Decay Test Files Analysis\n",
    "\n",
    "This notebook analyzes the completed CNN autoencoder training results to identify and extract the list of NPY files that were designated for testing but not used in training. This is crucial for understanding which data files can be used for further anomaly detection analysis.\n",
    "\n",
    "## Training Context\n",
    "- **Model**: CNN Autoencoder for Pi0 decay anomaly detection\n",
    "- **Training completed**: 60 epochs\n",
    "- **Results location**: `/nevis/riverside/data/sc5303/models/cnn_autoencoder_results_20250807_111552/`\n",
    "- **Test data preserved**: `test_events.pkl` (213MB), `test_events_info.json` (69KB)\n",
    "- **Dataset**: 483 NPY files containing physics event data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994f941f",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d1658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49639cec",
   "metadata": {},
   "source": [
    "## 2. Define File Paths and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23373e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training results directory\n",
    "results_dir = Path(\"/nevis/riverside/data/sc5303/models/cnn_autoencoder_results_20250807_111552\")\n",
    "\n",
    "# Original data directory\n",
    "data_dir = Path(\"/nevis/riverside/data/sc5303/data/pi0_decay\")\n",
    "\n",
    "# Test events info file\n",
    "test_info_file = results_dir / \"test_events_info.json\"\n",
    "test_events_file = results_dir / \"test_events.pkl\"\n",
    "\n",
    "# Output file for unused NPY files list\n",
    "output_file = Path.cwd() / \"unused_test_files.txt\"\n",
    "\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Test info file: {test_info_file}\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"\\nFile existence check:\")\n",
    "print(f\"Test info file exists: {test_info_file.exists()}\")\n",
    "print(f\"Test events file exists: {test_events_file.exists()}\")\n",
    "print(f\"Data directory exists: {data_dir.exists()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cea303",
   "metadata": {},
   "source": [
    "## 3. Load Test Data Information\n",
    "\n",
    "Load the test events information to understand which files were used for testing and which event indices were selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ffb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test events info\n",
    "with open(test_info_file, 'r') as f:\n",
    "    test_info = json.load(f)\n",
    "\n",
    "# Extract test event indices and files used\n",
    "test_indices = test_info['test_indices']\n",
    "files_used = test_info['files_used']\n",
    "test_split_seed = test_info['test_split_seed']\n",
    "\n",
    "print(f\"Test split seed: {test_split_seed}\")\n",
    "print(f\"Number of test events: {len(test_indices)}\")\n",
    "print(f\"Number of files used in training: {len(files_used)}\")\n",
    "\n",
    "print(f\"\\nFirst 10 test event indices: {test_indices[:10]}\")\n",
    "print(f\"First 5 files used in training: {files_used[:5]}\")\n",
    "\n",
    "# Create a set of training files for faster lookup\n",
    "training_files_set = set(files_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c6a7b",
   "metadata": {},
   "source": [
    "## 4. Scan Original Dataset for All NPY Files\n",
    "\n",
    "Find all NPY files in the original dataset directory to get the complete list of available files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all NPY files in the data directory\n",
    "npy_pattern = str(data_dir / \"*.npy\")\n",
    "all_npy_files = glob.glob(npy_pattern)\n",
    "\n",
    "# Extract just the filenames (not full paths)\n",
    "all_npy_filenames = [Path(f).name for f in all_npy_files]\n",
    "\n",
    "print(f\"Total NPY files found in dataset: {len(all_npy_filenames)}\")\n",
    "print(f\"First 5 NPY files: {all_npy_filenames[:5]}\")\n",
    "\n",
    "# Create a set for faster lookup\n",
    "all_files_set = set(all_npy_filenames)\n",
    "\n",
    "# Verify that training files are subset of all files\n",
    "training_files_in_dataset = training_files_set.intersection(all_files_set)\n",
    "missing_training_files = training_files_set - all_files_set\n",
    "\n",
    "print(f\"\\nTraining files found in dataset: {len(training_files_in_dataset)}\")\n",
    "print(f\"Training files missing from dataset: {len(missing_training_files)}\")\n",
    "\n",
    "if missing_training_files:\n",
    "    print(f\"Missing training files: {list(missing_training_files)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bba9c3",
   "metadata": {},
   "source": [
    "## 5. Identify Files Not Used in Training (Available for Testing)\n",
    "\n",
    "Compare the complete dataset with training files to identify files that were not used in training and are available for testing/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d772443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find files that were NOT used in training (available for testing)\n",
    "unused_files = all_files_set - training_files_set\n",
    "unused_files_list = sorted(list(unused_files))\n",
    "\n",
    "print(f\"Files NOT used in training (available for testing): {len(unused_files_list)}\")\n",
    "print(f\"Files used in training: {len(training_files_set)}\")\n",
    "print(f\"Total files in dataset: {len(all_files_set)}\")\n",
    "\n",
    "# Verify the math\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Used + Unused = {len(training_files_set)} + {len(unused_files_list)} = {len(training_files_set) + len(unused_files_list)}\")\n",
    "print(f\"Total files = {len(all_files_set)}\")\n",
    "print(f\"Math checks out: {len(training_files_set) + len(unused_files_list) == len(all_files_set)}\")\n",
    "\n",
    "# Show some examples of unused files\n",
    "print(f\"\\nFirst 10 unused files:\")\n",
    "for i, file in enumerate(unused_files_list[:10]):\n",
    "    print(f\"  {i+1}. {file}\")\n",
    "\n",
    "print(f\"\\nLast 10 unused files:\")\n",
    "for i, file in enumerate(unused_files_list[-10:], len(unused_files_list)-9):\n",
    "    print(f\"  {i}. {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d7abe8",
   "metadata": {},
   "source": [
    "## 6. Load Test Events Data for Detailed Analysis\n",
    "\n",
    "Load the actual test events data to understand the structure and content of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c902da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test events data\n",
    "with open(test_events_file, 'rb') as f:\n",
    "    test_events = pickle.load(f)\n",
    "\n",
    "print(f\"Test events data type: {type(test_events)}\")\n",
    "print(f\"Test events shape: {test_events.shape if hasattr(test_events, 'shape') else 'N/A'}\")\n",
    "\n",
    "# If it's a numpy array, show some statistics\n",
    "if isinstance(test_events, np.ndarray):\n",
    "    print(f\"Test events dtype: {test_events.dtype}\")\n",
    "    print(f\"Test events size: {test_events.size}\")\n",
    "    print(f\"Memory usage: {test_events.nbytes / (1024*1024):.2f} MB\")\n",
    "    \n",
    "    # Show shape information for different dimensions\n",
    "    if len(test_events.shape) > 1:\n",
    "        print(f\"Shape breakdown:\")\n",
    "        for i, dim in enumerate(test_events.shape):\n",
    "            print(f\"  Dimension {i}: {dim}\")\n",
    "\n",
    "# Show relationship between test indices and test events\n",
    "print(f\"\\nData relationship:\")\n",
    "print(f\"Number of test indices: {len(test_indices)}\")\n",
    "print(f\"Test events first dimension: {test_events.shape[0] if hasattr(test_events, 'shape') else 'N/A'}\")\n",
    "print(f\"Do they match? {len(test_indices) == test_events.shape[0] if hasattr(test_events, 'shape') else 'Unknown'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78161a10",
   "metadata": {},
   "source": [
    "## 7. Save Results to Files\n",
    "\n",
    "Save the list of unused NPY files and comprehensive analysis results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ed556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save unused files list to text file\n",
    "unused_files_txt = Path.cwd() / \"unused_npy_files.txt\"\n",
    "with open(unused_files_txt, 'w') as f:\n",
    "    f.write(f\"# NPY Files Not Used in Training - Available for Testing\\n\")\n",
    "    f.write(f\"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"# Total unused files: {len(unused_files_list)}\\n\")\n",
    "    f.write(f\"# Training files: {len(training_files_set)}\\n\")\n",
    "    f.write(f\"# Total dataset files: {len(all_files_set)}\\n\\n\")\n",
    "    \n",
    "    for file in unused_files_list:\n",
    "        f.write(f\"{file}\\n\")\n",
    "\n",
    "print(f\"Unused files list saved to: {unused_files_txt}\")\n",
    "\n",
    "# Save training files list for comparison\n",
    "training_files_txt = Path.cwd() / \"training_npy_files.txt\"\n",
    "with open(training_files_txt, 'w') as f:\n",
    "    f.write(f\"# NPY Files Used in Training\\n\")\n",
    "    f.write(f\"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"# Total training files: {len(training_files_set)}\\n\\n\")\n",
    "    \n",
    "    for file in sorted(training_files_set):\n",
    "        f.write(f\"{file}\\n\")\n",
    "\n",
    "print(f\"Training files list saved to: {training_files_txt}\")\n",
    "\n",
    "# Save comprehensive analysis results\n",
    "analysis_results = {\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'total_dataset_files': len(all_files_set),\n",
    "    'training_files_count': len(training_files_set),\n",
    "    'unused_files_count': len(unused_files_list),\n",
    "    'test_events_count': len(test_indices),\n",
    "    'test_split_seed': test_split_seed,\n",
    "    'training_files': sorted(training_files_set),\n",
    "    'unused_files': unused_files_list,\n",
    "    'test_indices': test_indices\n",
    "}\n",
    "\n",
    "results_json = Path.cwd() / \"pi0_decay_analysis_results.json\"\n",
    "with open(results_json, 'w') as f:\n",
    "    json.dump(analysis_results, f, indent=2)\n",
    "\n",
    "print(f\"Comprehensive analysis saved to: {results_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005972f2",
   "metadata": {},
   "source": [
    "## 8. Summary and Final Statistics\n",
    "\n",
    "Display comprehensive statistics and summary of the analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d873d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "print(\"=\"*60)\n",
    "print(\"         PI0 DECAY CNN AUTOENCODER - TEST FILES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDATASET OVERVIEW:\")\n",
    "print(f\"  Total NPY files in dataset: {len(all_files_set)}\")\n",
    "print(f\"  Files used for training: {len(training_files_set)} ({len(training_files_set)/len(all_files_set)*100:.1f}%)\")\n",
    "print(f\"  Files available for testing: {len(unused_files_list)} ({len(unused_files_list)/len(all_files_set)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTRAINING DETAILS:\")\n",
    "print(f\"  Test split seed: {test_split_seed}\")\n",
    "print(f\"  Number of test events: {len(test_indices)}\")\n",
    "print(f\"  Test events file size: {test_events_file.stat().st_size / (1024*1024):.1f} MB\")\n",
    "\n",
    "print(f\"\\nFILE STATISTICS:\")\n",
    "print(f\"  Unused files (first 20):\")\n",
    "for i, file in enumerate(unused_files_list[:20]):\n",
    "    print(f\"    {i+1:2d}. {file}\")\n",
    "\n",
    "if len(unused_files_list) > 20:\n",
    "    print(f\"    ... and {len(unused_files_list) - 20} more files\")\n",
    "\n",
    "print(f\"\\nOUTPUT FILES GENERATED:\")\n",
    "print(f\"  1. {unused_files_txt.name} - List of {len(unused_files_list)} unused NPY files\")\n",
    "print(f\"  2. {training_files_txt.name} - List of {len(training_files_set)} training NPY files\")\n",
    "print(f\"  3. {results_json.name} - Comprehensive analysis in JSON format\")\n",
    "\n",
    "print(f\"\\nNEXT STEPS:\")\n",
    "print(f\"  • Use unused NPY files for additional testing/validation\")\n",
    "print(f\"  • Implement anomaly detection on test dataset\")\n",
    "print(f\"  • Compare results with sparse graph autoencoder\")\n",
    "print(f\"  • Analyze model performance on unseen data\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"                        ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
